{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Classification Technical Report\n",
    "Authors: Suyash Kushwaha and Jack Brandt  \n",
    "\n",
    "Course: CPSC 322\n",
    "\n",
    "Assignment: Final Project\n",
    "\n",
    "Date of current version: 1?/??/2024\n",
    "\n",
    "Did you attempt the bonus? ???\n",
    "\n",
    "Brief description of project goals:\n",
    "\n",
    "* Classify running speed based off other metrics\n",
    "* Learn about random forest classification\n",
    "* idk, have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import importlib\n",
    "# mypytable\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "# myevaluation\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as me\n",
    "# myutils\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as mu\n",
    "\n",
    "from mysklearn.myutils import combine_multiple_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The data came as a large number of JSON files grouped into folders. Many of the files and folders were empty, so we began by deleting those. Some of the files contained irrelevant data/non-changing data, such as date of birth and device specs, so we also deleted those. What we were left with was three sets of JSON files, aggregator, fitness, and wellness.\n",
    "\n",
    "* Aggregator: This contains the most info. It has a list of metrics related to stress, calories, heart rate, and minor metrics related to activity\n",
    "* Fitness: This contains a list of activities and information about them. All of these activities are runs. Has attributes like distance, speed, heart rate, duration\n",
    "* Wellness: This mainly contained sleep data\n",
    "\n",
    "### Cleaning/Joining TODO\n",
    "\n",
    "Many instances in the dataset are missing values, or are just instances with basically no data. TODO: Make copies of the data without these bad instances\n",
    "\n",
    "Additionally, for this step we combine all of the many JSON files into one. First by joining all files within each folder, simply appending them onto each other. Then join the lists from each folder. \n",
    "\n",
    "We also opened the CSV's in Excel to guide our decision-making process for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning/Joining...\n",
    "# SLEEP\n",
    "# 1. load all files\n",
    "\n",
    "sleep_file_names = [\n",
    "    \"2021-04-23_2021-08-01_96200873_sleepData.csv\",\n",
    "    \"2021-08-01_2021-11-09_96200873_sleepData.csv\",\n",
    "    \"2021-11-09_2022-02-17_96200873_sleepData.csv\",\n",
    "    \"2022-02-17_2022-05-28_96200873_sleepData.csv\",\n",
    "    \"2022-05-28_2022-09-05_96200873_sleepData.csv\",\n",
    "    \"2022-09-05_2022-12-14_96200873_sleepData.csv\",\n",
    "    \"2022-12-14_2023-03-24_96200873_sleepData.csv\",\n",
    "    \"2023-03-24_2023-07-02_96200873_sleepData.csv\",\n",
    "    \"2023-07-02_2023-10-10_96200873_sleepData.csv\",\n",
    "    \"2023-10-10_2024-01-18_96200873_sleepData.csv\",\n",
    "    \"2024-01-18_2024-04-27_96200873_sleepData.csv\",\n",
    "    \"2024-04-27_2024-08-05_96200873_sleepData.csv\",\n",
    "    \"2024-08-05_2024-11-13_96200873_sleepData.csv\"\n",
    "]\n",
    "\n",
    "full_sleep_table = combine_multiple_files(sleep_file_names, \"csv_converted_data/connect_wellness\")\n",
    "\n",
    "#current_table.pretty_print()\n",
    "# 3. Basic cleaning\n",
    "full_sleep_table.remove_rows_with_missing_values()\n",
    "full_sleep_table.remove_rows_where_col_equal_specified(\n",
    "    full_sleep_table.column_names.index(\"sleepWindowConfirmationType\"),\n",
    "    'OFF_WRIST'\n",
    ")\n",
    "\n",
    "# This is all of our joined sleep data\n",
    "#full_sleep_table.pretty_print()\n",
    "full_sleep_table.save_to_file('joined_nullfree_subsets/full_sleep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for other file sets\n",
    "# ACTIVITY\n",
    "\n",
    "acitivity_files_names = [\n",
    "    \"jack-brandt@comcast.net_0_summarizedActivities.csv\",\n",
    "    \"jack-brandt@comcast.net_1001_summarizedActivities.csv\"\n",
    "]\n",
    "full_activity_table = combine_multiple_files(acitivity_files_names, \"csv_converted_data/connect_fitness\")\n",
    "full_activity_table.save_to_file('joined_nullfree_subsets/full_activity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for other file sets\n",
    "# AGGREGATOR\n",
    "\n",
    "aggregator_file_names = [\n",
    "    \"UDSFile_2021-04-23_2021-08-01.csv\",\n",
    "    \"UDSFile_2021-08-01_2021-11-09.csv\",\n",
    "    \"UDSFile_2021-11-09_2022-02-17.csv\",\n",
    "    \"UDSFile_2022-02-17_2022-05-28.csv\",\n",
    "    \"UDSFile_2022-05-28_2022-09-05.csv\",\n",
    "    \"UDSFile_2022-09-05_2022-12-14.csv\",\n",
    "    \"UDSFile_2022-12-14_2023-03-24.csv\",\n",
    "    \"UDSFile_2023-03-24_2023-07-02.csv\",\n",
    "    \"UDSFile_2023-07-02_2023-10-10.csv\",\n",
    "    \"UDSFile_2023-10-10_2024-01-18.csv\",\n",
    "    \"UDSFile_2024-01-18_2024-04-27.csv\",\n",
    "    \"UDSFile_2024-04-27_2024-08-05.csv\",\n",
    "    \"UDSFile_2024-08-05_2024-11-13.csv\",\n",
    "]\n",
    "\n",
    "full_aggregator_table = combine_multiple_files(aggregator_file_names, \"csv_converted_data/connect_aggregator_data\")\n",
    "full_aggregator_table.save_to_file('joined_nullfree_subsets/full_aggregator.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the combined tables\n",
    "full_activity_table = MyPyTable().load_from_file(\n",
    "    \"joined_nullfree_subsets/full_activity.csv\"\n",
    ")\n",
    "full_sleep_table = MyPyTable().load_from_file(\"joined_nullfree_subsets/full_sleep.csv\")\n",
    "full_aggregator_table = MyPyTable().load_from_file(\"joined_nullfree_subsets/full_aggregator.csv\")\n",
    "\n",
    "# full_activity_table doesn't have a calendarDate column, so we need to add it\n",
    "# by converting the startTimeLocal column to a date\n",
    "start_time_index = full_activity_table.column_names.index(\"startTimeLocal\")\n",
    "\n",
    "# Add the column\n",
    "full_activity_table.column_names.append(\"calendarDate\")\n",
    "\n",
    "# Add the calendarDate to each row\n",
    "for data in full_activity_table.data:\n",
    "    timestamp = data[start_time_index] / 1000\n",
    "    dt_object = datetime.fromtimestamp(timestamp)\n",
    "    date = dt_object.strftime(\"%Y-%m-%d\")\n",
    "    data.append(date)\n",
    "\n",
    "# Now we can join the tables\n",
    "fully_joined_table = (full_activity_table.perform_inner_join(\n",
    "    full_sleep_table, [\"calendarDate\"]\n",
    ")).perform_inner_join(\n",
    "    full_aggregator_table, [\"calendarDate\"]\n",
    ")\n",
    "\n",
    "stress_dict = {}\n",
    "calendar_date_index = full_aggregator_table.column_names.index(\"calendarDate\")\n",
    "stress_index = full_aggregator_table.column_names.index(\"allDayStress/aggregatorList/0/maxStressLevel\")\n",
    "\n",
    "\n",
    "for row in full_aggregator_table.data:\n",
    "    stress_dict[row[calendar_date_index]] = row[stress_index]\n",
    "\n",
    "# Add the stress level to the fully joined table\n",
    "fully_joined_table.column_names.append(\"prevDayMaxStressLevel\")\n",
    "fully_joined_table_calender_date_index = fully_joined_table.column_names.index(\"calendarDate\")\n",
    "\n",
    "for row in fully_joined_table.data:\n",
    "    calendar_date = row[fully_joined_table_calender_date_index]\n",
    "\n",
    "    # Get the previous calendar date\n",
    "    prev_calendar_date = (datetime.strptime(calendar_date, \"%Y-%m-%d\") - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Get the stress level for the previous day\n",
    "    if prev_calendar_date in stress_dict:\n",
    "        row.append(stress_dict[prev_calendar_date])\n",
    "    else:\n",
    "        row.append(None)\n",
    "\n",
    "fully_joined_table.save_to_file(\"processed_data/fully_joined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute                      min           max           mid            avg        median\n",
      "---------------------  -----------  ------------  ------------  -------------  ------------\n",
      "prevDayMaxStressLevel      60        99            79.5          91.7749        92\n",
      "avgHr                     125       176           150.5         153.048        153\n",
      "duration               633362         1.3104e+07    6.8687e+06    2.35486e+06    2.2341e+06\n",
      "avgSpeed                    0.1722    0.630717      0.401458      0.328749       0.3279\n"
     ]
    }
   ],
   "source": [
    "columns = [\"prevDayMaxStressLevel\", \"avgHr\", \"duration\", \"avgSpeed\"]\n",
    "class_column = \"avgSpeed\"\n",
    "column_index = {}\n",
    "\n",
    "def speed_discretizer(speed):\n",
    "    if speed < 0.32:\n",
    "        return \"slow\"\n",
    "    elif speed < 0.33:\n",
    "        return \"mild\"\n",
    "    else:\n",
    "        return \"fast\"\n",
    "\n",
    "def heart_rate_discretizer(bpm):\n",
    "    if(bpm < 150):\n",
    "        return \"low\"\n",
    "    elif(bpm < 165):\n",
    "        return \"mid\"\n",
    "\n",
    "    return \"high\"\n",
    "\n",
    "def stress_discretizer(stress):\n",
    "    if(stress < 90):\n",
    "        return \"low\"\n",
    "    elif(stress < 95):\n",
    "        return \"mid\"\n",
    "\n",
    "    return \"high\"\n",
    "\n",
    "def duration_discretizer(duration):\n",
    "    if(duration < 2_000_000):\n",
    "        return \"low\"\n",
    "    elif(duration < 4_000_000):\n",
    "        return \"mid\"\n",
    "\n",
    "    return \"high\"\n",
    "\n",
    "for column in columns:\n",
    "    column_index[column] = fully_joined_table.column_names.index(column)\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in fully_joined_table.data:\n",
    "    data.append([row[column_index[column]] for column in columns])\n",
    "\n",
    "table = MyPyTable(column_names=columns, data=data)\n",
    "\n",
    "# Remove rows with missing values\n",
    "table.remove_rows_with_missing_values()\n",
    "\n",
    "# Remove runs that are less than 10 minutes\n",
    "table.remove_row_if(table.get_index(\"duration\"), lambda x: x < 600000)\n",
    "table.remove_rows_where_col_equal_specified(table.get_index(class_column), 0)\n",
    "\n",
    "columns_to_extract = columns.copy()\n",
    "columns_to_extract.remove(class_column)\n",
    "\n",
    "table.save_to_file(\"processed_data/processed_data.csv\")\n",
    "\n",
    "X = [[stress_discretizer(row[0]), heart_rate_discretizer(row[1]), duration_discretizer(row[2])] for row in table.get_data_subset(columns_to_extract, False)]\n",
    "X_undiscretized = table.get_data_subset(columns_to_extract, False)\n",
    "y = [speed_discretizer(row) for row in table.get_column(class_column)]\n",
    "\n",
    "\n",
    "table.compute_summary_statistics(table.column_names).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Dummy Summary Results\n",
      "===========================================\n",
      "1.\n",
      "    Accuracy: 0.4320827943078913\n",
      "    Error Rate: 0.5679172056921087\n",
      "2.\n",
      "    Precision: 0.4320827943078913\n",
      "    Recall: 1.0\n",
      "    F1 measure: 0.6034327009936766\n",
      "3. Confusion Matrix:\n",
      "Running      slow    mild    fast    Total    Recognition (%)\n",
      "---------  ------  ------  ------  -------  -----------------\n",
      "slow            0       0     198      198                  0\n",
      "mild            0       0     241      241                  0\n",
      "fast            0       0     334      334                100\n",
      "(Bonus) Classification Report:\n",
      "              precision    recall    f1-score    support\n",
      "------------  -----------  --------  ----------  ---------\n",
      "slow          0.0          0.0       0           198\n",
      "mild          0.0          0.0       0           241\n",
      "fast          0.43         1.0       0.6         334\n",
      "\n",
      "micro avg     0.43         1.0       0.6         773\n",
      "macro avg     0.14         0.33      0.2         773\n",
      "weighted avg  0.19         0.43      0.26        773\n"
     ]
    }
   ],
   "source": [
    "#Set up models # Add more from myclassifiers if time\n",
    "from mysklearn.myclassifiers import MyDecisionTreeClassifier, MyDummyClassifier, MyKNeighborsClassifier, MyNaiveBayesClassifier, MyRandomForestClassifier\n",
    "\n",
    "\n",
    "dummy_model = MyDummyClassifier() # Import these from myclassifiers\n",
    "knn_model = MyKNeighborsClassifier()\n",
    "bayes_model = MyNaiveBayesClassifier()\n",
    "tree_model = MyDecisionTreeClassifier()\n",
    "forest_model = MyRandomForestClassifier()\n",
    "\n",
    "labels = ['slow','mild','fast']\n",
    "pos_label='fast'\n",
    "\n",
    "# Repeat these following two lines for each possible model\n",
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(dummy_model,10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('Dummy',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "KNN Summary Results\n",
      "===========================================\n",
      "1.\n",
      "    Accuracy: 0.6688227684346701\n",
      "    Error Rate: 0.33117723156532985\n",
      "2.\n",
      "    Precision: 0.7277936962750716\n",
      "    Recall: 0.7604790419161677\n",
      "    F1 measure: 0.7437774524158126\n",
      "3. Confusion Matrix:\n",
      "Running      slow    mild    fast    Total    Recognition (%)\n",
      "---------  ------  ------  ------  -------  -----------------\n",
      "slow          118      42      38      198            59.596\n",
      "mild           39     145      57      241            60.166\n",
      "fast           30      50     254      334            76.0479\n",
      "(Bonus) Classification Report:\n",
      "              precision    recall    f1-score    support\n",
      "------------  -----------  --------  ----------  ---------\n",
      "slow          0.63         0.6       0.61        198\n",
      "mild          0.61         0.6       0.61        241\n",
      "fast          0.73         0.76      0.74        334\n",
      "\n",
      "accuracy                             0.67        773\n",
      "macro avg     0.66         0.65      0.65        773\n",
      "weighted avg  0.67         0.67      0.67        773\n"
     ]
    }
   ],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(knn_model,10,X_undiscretized,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('KNN',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Bayes Summary Results\n",
      "===========================================\n",
      "1.\n",
      "    Accuracy: 0.48253557567917205\n",
      "    Error Rate: 0.517464424320828\n",
      "2.\n",
      "    Precision: 0.5371549893842887\n",
      "    Recall: 0.7574850299401198\n",
      "    F1 measure: 0.6285714285714286\n",
      "3. Confusion Matrix:\n",
      "Running      slow    mild    fast    Total    Recognition (%)\n",
      "---------  ------  ------  ------  -------  -----------------\n",
      "slow           99      25      74      198           50\n",
      "mild           76      21     144      241            8.71369\n",
      "fast           71      10     253      334           75.7485\n",
      "(Bonus) Classification Report:\n",
      "              precision    recall    f1-score    support\n",
      "------------  -----------  --------  ----------  ---------\n",
      "slow          0.4          0.5       0.45        198\n",
      "mild          0.38         0.09      0.14        241\n",
      "fast          0.54         0.76      0.63        334\n",
      "\n",
      "accuracy                             0.48        773\n",
      "macro avg     0.44         0.45      0.41        773\n",
      "weighted avg  0.45         0.48      0.43        773\n"
     ]
    }
   ],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(MyNaiveBayesClassifier(),10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('Bayes',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Tree Summary Results\n",
      "===========================================\n",
      "1.\n",
      "    Accuracy: 0.47089262613195343\n",
      "    Error Rate: 0.5291073738680465\n",
      "2.\n",
      "    Precision: 0.5310344827586206\n",
      "    Recall: 0.6916167664670658\n",
      "    F1 measure: 0.6007802340702211\n",
      "3. Confusion Matrix:\n",
      "Running      slow    mild    fast    Total    Recognition (%)\n",
      "---------  ------  ------  ------  -------  -----------------\n",
      "slow           96      34      68      198            48.4848\n",
      "mild           68      37     136      241            15.3527\n",
      "fast           64      39     231      334            69.1617\n",
      "(Bonus) Classification Report:\n",
      "              precision    recall    f1-score    support\n",
      "------------  -----------  --------  ----------  ---------\n",
      "slow          0.42         0.48      0.45        198\n",
      "mild          0.34         0.15      0.21        241\n",
      "fast          0.53         0.69      0.6         334\n",
      "\n",
      "accuracy                             0.47        773\n",
      "macro avg     0.43         0.44      0.42        773\n",
      "weighted avg  0.44         0.47      0.44        773\n"
     ]
    }
   ],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(tree_model,10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('Tree',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Forest Summary Results\n",
      "===========================================\n",
      "1.\n",
      "    Accuracy: 0.4760672703751617\n",
      "    Error Rate: 0.5239327296248383\n",
      "2.\n",
      "    Precision: 0.5129224652087475\n",
      "    Recall: 0.7724550898203593\n",
      "    F1 measure: 0.6164874551971327\n",
      "3. Confusion Matrix:\n",
      "Running      slow    mild    fast    Total    Recognition (%)\n",
      "---------  ------  ------  ------  -------  -----------------\n",
      "slow          110       0      88      198            55.5556\n",
      "mild           84       0     157      241             0\n",
      "fast           76       0     258      334            77.2455\n",
      "(Bonus) Classification Report:\n",
      "              precision    recall    f1-score    support\n",
      "------------  -----------  --------  ----------  ---------\n",
      "slow          0.41         0.56      0.47        198\n",
      "mild          0.0          0.0       0           241\n",
      "fast          0.51         0.77      0.62        334\n",
      "\n",
      "accuracy                             0.48        773\n",
      "macro avg     0.31         0.44      0.36        773\n",
      "weighted avg  0.33         0.48      0.39        773\n"
     ]
    }
   ],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report_forest(forest_model,10,X,y,labels,pos_label,'Running', N=13, M=11, F=1)\n",
    "mu.report_metrics_and_confusion('Forest',metrics, confusion,\n",
    "    clas_repor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
