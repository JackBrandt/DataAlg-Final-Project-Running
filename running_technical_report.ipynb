{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT REPORT TITLE AND INFO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import importlib\n",
    "# mypytable\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "# myevaluation\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as me\n",
    "# myutils\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as mu\n",
    "\n",
    "from mysklearn.myutils import combine_multiple_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The data came as a large number of JSON files grouped into folders. Many of the files and folders were empty, so we began by deleting those. Some of the files contained irrelevant data/non-changing data, such as date of birth and device specs, so we also deleted those. What we were left with was three sets of JSON files, aggregator, fitness, and wellness.\n",
    "\n",
    "* Aggregator: This contains the most info. It has a list of metrics related to stress, calories, heart rate, and minor metrics related to activity\n",
    "* Fitness: This contains a list of activities and information about them. All of these activities are runs. Has attributes like distance, speed, heart rate, duration\n",
    "* Wellness: This mainly contained sleep data\n",
    "\n",
    "### Cleaning/Joining TODO\n",
    "\n",
    "Many instances in the dataset are missing values, or are just instances with basically no data. TODO: Make copies of the data without these bad instances\n",
    "\n",
    "Additionally, for this step we combine all of the many JSON files into one. First by joining all files within each folder, simply appending them onto each other. Then join the lists from each folder. \n",
    "\n",
    "We also opened the CSV's in Excel to guide our decision-making process for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning/Joining...\n",
    "# SLEEP\n",
    "# 1. load all files\n",
    "\n",
    "sleep_file_names = [\n",
    "    \"2021-04-23_2021-08-01_96200873_sleepData.csv\",\n",
    "    \"2021-08-01_2021-11-09_96200873_sleepData.csv\",\n",
    "    \"2021-11-09_2022-02-17_96200873_sleepData.csv\",\n",
    "    \"2022-02-17_2022-05-28_96200873_sleepData.csv\",\n",
    "    \"2022-05-28_2022-09-05_96200873_sleepData.csv\",\n",
    "    \"2022-09-05_2022-12-14_96200873_sleepData.csv\",\n",
    "    \"2022-12-14_2023-03-24_96200873_sleepData.csv\",\n",
    "    \"2023-03-24_2023-07-02_96200873_sleepData.csv\",\n",
    "    \"2023-07-02_2023-10-10_96200873_sleepData.csv\",\n",
    "    \"2023-10-10_2024-01-18_96200873_sleepData.csv\",\n",
    "    \"2024-01-18_2024-04-27_96200873_sleepData.csv\",\n",
    "    \"2024-04-27_2024-08-05_96200873_sleepData.csv\",\n",
    "    \"2024-08-05_2024-11-13_96200873_sleepData.csv\"   \n",
    "]\n",
    "\n",
    "full_sleep_table = combine_multiple_files(sleep_file_names, \"csv_converted_data/connect_wellness\")\n",
    "\n",
    "#current_table.pretty_print()\n",
    "# 3. Basic cleaning\n",
    "full_sleep_table.remove_rows_with_missing_values()\n",
    "full_sleep_table.remove_rows_where_col_equal_specified(3,'OFF_WRIST')\n",
    "\n",
    "# This is all of our joined sleep data\n",
    "#full_sleep_table.pretty_print()\n",
    "full_sleep_table.save_to_file('joined_nullfree_subsets/full_sleep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for other file sets\n",
    "# ACTIVITY\n",
    "\n",
    "acitivity_files_names = [\n",
    "    \"jack-brandt@comcast.net_0_summarizedActivities.csv\",\n",
    "    \"jack-brandt@comcast.net_1001_summarizedActivities.csv\"\n",
    "]\n",
    "full_activity_table = combine_multiple_files(acitivity_files_names, \"csv_converted_data/connect_fitness\")\n",
    "full_activity_table.save_to_file('joined_nullfree_subsets/full_activity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for other file sets\n",
    "# AGGREGATOR\n",
    "\n",
    "aggregator_file_names = [\n",
    "    \"UDSFile_2021-04-23_2021-08-01.csv\",\n",
    "    \"UDSFile_2021-08-01_2021-11-09.csv\",\n",
    "    \"UDSFile_2021-11-09_2022-02-17.csv\",\n",
    "    \"UDSFile_2022-02-17_2022-05-28.csv\",\n",
    "    \"UDSFile_2022-05-28_2022-09-05.csv\",\n",
    "    \"UDSFile_2022-09-05_2022-12-14.csv\",\n",
    "    \"UDSFile_2022-12-14_2023-03-24.csv\",\n",
    "    \"UDSFile_2023-03-24_2023-07-02.csv\",\n",
    "    \"UDSFile_2023-07-02_2023-10-10.csv\",\n",
    "    \"UDSFile_2023-10-10_2024-01-18.csv\",\n",
    "    \"UDSFile_2024-01-18_2024-04-27.csv\",\n",
    "    \"UDSFile_2024-04-27_2024-08-05.csv\",\n",
    "    \"UDSFile_2024-08-05_2024-11-13.csv\",\n",
    "]\n",
    "\n",
    "full_aggregator_table = combine_multiple_files(aggregator_file_names, \"csv_converted_data/connect_aggregator_data\")\n",
    "full_aggregator_table.save_to_file('joined_nullfree_subsets/full_aggregator.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "9\n",
      "60\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m intermid_table \u001b[38;5;241m=\u001b[39m full_activity_table\u001b[38;5;241m.\u001b[39mperform_inner_join(\n\u001b[1;32m     33\u001b[0m     full_sleep_table, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalendarDate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(intermid_table\u001b[38;5;241m.\u001b[39mcolumn_names))\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(intermid_table\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     38\u001b[0m intermid_table\u001b[38;5;241m.\u001b[39msave_to_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data/intermid_table.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "full_activity_table = MyPyTable().load_from_file(\n",
    "    \"joined_nullfree_subsets/full_activity.csv\"\n",
    ")\n",
    "full_sleep_table = MyPyTable().load_from_file(\"joined_nullfree_subsets/full_sleep.csv\")\n",
    "full_aggregator_table = MyPyTable().load_from_file(\"joined_nullfree_subsets/full_aggregator.csv\")\n",
    "\n",
    "\n",
    "start_time_index = full_activity_table.column_names.index(\"startTimeLocal\")\n",
    "\n",
    "full_activity_table.column_names.append(\"calendarDate\")\n",
    "\n",
    "for data in full_activity_table.data:\n",
    "    timestamp = data[start_time_index] / 1000\n",
    "    dt_object = datetime.fromtimestamp(timestamp)\n",
    "    date = dt_object.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if len(data) == 62:\n",
    "        data.append(date)\n",
    "    else:\n",
    "        for i in range(62 - len(data)):\n",
    "            data.append(None)\n",
    "        data.append(date)\n",
    "\n",
    "full_activity_table.save_to_file(\"processed_data/full_activity.csv\")\n",
    "full_sleep_table.save_to_file(\"processed_data/full_sleep.csv\")\n",
    "\n",
    "print(len(full_activity_table.column_names))\n",
    "print(len(full_sleep_table.column_names))\n",
    "\n",
    "intermid_table = full_activity_table.perform_inner_join(\n",
    "    full_sleep_table, [\"calendarDate\"]\n",
    ")\n",
    "\n",
    "print(len(intermid_table.column_names))\n",
    "print(len(intermid_table.data[0]))\n",
    "intermid_table.save_to_file(\"processed_data/intermid_table.csv\")\n",
    "\n",
    "# fully_joined_table = intermid_table.perform_inner_join(full_aggregator_table, [\"calendarDate\"])\n",
    "\n",
    "# fully_joined_table.save_to_file(\"processed_data/fully_joined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and discretize!\n",
    "\n",
    "# Save discretized data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyDummyClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Set up models # Add more from myclassifiers if time\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dummy_model \u001b[38;5;241m=\u001b[39m MyDummyClassifier() \u001b[38;5;66;03m# Import these from myclassifiers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m knn_model \u001b[38;5;241m=\u001b[39m MyKNeighborsClassifier()\n\u001b[1;32m      4\u001b[0m bayes_model \u001b[38;5;241m=\u001b[39m MyNaiveBayesClassifier()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyDummyClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "#Set up models # Add more from myclassifiers if time\n",
    "dummy_model = MyDummyClassifier() # Import these from myclassifiers\n",
    "knn_model = MyKNeighborsClassifier()\n",
    "bayes_model = MyNaiveBayesClassifier()\n",
    "tree_model = MyDecisionTreeClassifier()\n",
    "forest_model = MyRandomForestClassifier()\n",
    "\n",
    "# Set up X\n",
    "final_table = MyPyTable()\n",
    "final_table.load_from_file('?')\n",
    "X = final_table.get_data_subset(['INSERT IMPORTANT ATTRIBUTES HERE'],False)\n",
    "y = final_table.get_column('DISCRETIZED Y COLUMN HERE')\n",
    "# Labels\n",
    "labels = ['slow','mild','fast']\n",
    "# Positive Label\n",
    "pos_label='fast'\n",
    "\n",
    "# Repeat these following two lines for each possible model\n",
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(dummy_model,10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('Dummy',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(knn_model,10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('KNN',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(tree_model,10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('Tree',metrics, confusion,\n",
    "    clas_repor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, confusion, clas_repor = me.get_metrics_and_conf_matrix_and_report(forest_model,10,X,y,labels,pos_label,'Running')\n",
    "mu.report_metrics_and_confusion('Forest',metrics, confusion,\n",
    "    clas_repor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
